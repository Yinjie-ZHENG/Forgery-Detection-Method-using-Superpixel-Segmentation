{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667d93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/home/yinjie/JPEG-project/dataset/step2dir/comp_image.jpg\"\n",
    "comb_mask_dir = \"/home/yinjie/JPEG-project/dataset/step2dir/combined_mask.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25c9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "import torchjpeg.codec\n",
    "from fast_histogram import histogram1d\n",
    "\n",
    "sys.path.append('/home/yinjie/JPEG-project/utils')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a458559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(hist, bin_range):\n",
    "    bins = np.arange(-bin_range, bin_range+2)\n",
    "    width = 0.7 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.show()\n",
    "    \n",
    "def crop_leave4(im):\n",
    "    return im.crop((4, 4, im.size[0]-4, im.size[1]-4))\n",
    "\n",
    "def chi2_hist_distance(h1, h2):\n",
    "    distance = 0\n",
    "    for b in range(len(h1)):\n",
    "        if h1[b] == 0 and h2[b] == 0:\n",
    "            continue\n",
    "        distance += (h1[b] - h2[b])**2 / (h1[b] + h2[b])\n",
    " \n",
    "    return distance\n",
    "\n",
    "def get_closest_histogram(reference, histogram_list):\n",
    "    smallest_dist = np.inf\n",
    "    smallest_idx = -1\n",
    "    closest_hist = None\n",
    "    \n",
    "    for idx, h in enumerate(histogram_list):\n",
    "        dist = chi2_hist_distance(h, reference)\n",
    "        \n",
    "        if dist < smallest_dist:\n",
    "            smallest_dist = dist\n",
    "            smallest_idx = idx\n",
    "            closest_hist = h\n",
    "    \n",
    "    return closest_hist, smallest_idx\n",
    "\n",
    "def compute_dct_coefficient_histogram(dct_blocks, bin_range=50):\n",
    "    k_factor_list = dct_blocks.reshape(-1, 64).transpose()\n",
    "    \n",
    "    histograms = []\n",
    "    for k in k_factor_list:\n",
    "        h = histogram1d(k, bins=bin_range*2+1, range=(-bin_range,bin_range+1))\n",
    "        histograms.append(h)\n",
    "\n",
    "    return np.array(histograms)\n",
    "\n",
    "def get_first_n_percentage_diff(q1, q2, n):\n",
    "    q1_first_n = q1[ZIGZAG_ROW_IDX[:n], ZIGZAG_COL_IDX[:n]]\n",
    "    q2_first_n = q2[ZIGZAG_ROW_IDX[:n], ZIGZAG_COL_IDX[:n]]\n",
    "    diff = q1_first_n - q2_first_n\n",
    "    percentage_diff = np.count_nonzero(diff) / n * 100\n",
    "    return percentage_diff\n",
    "\n",
    "def compute_dct_blocks(tampered_path, gt_path):\n",
    "    # Read tampered image as dct blocks\n",
    "    dims, mth_q_tables, quantized_dct_blocks, _ = torchjpeg.codec.read_coefficients(tampered_path)\n",
    "    mth_q_table_lumi = mth_q_tables[0]\n",
    "    dct_blocks = (quantized_dct_blocks * mth_q_table_lumi).squeeze()\n",
    "    \n",
    "    # Read ground truth mask\n",
    "    gt_mask = to_tensor(Image.open(gt_path).convert('L')).squeeze()  # PIL convert(\"L\") remove unnecessary colour/alpha channels\n",
    "    \n",
    "    h = dims[0,0].item()\n",
    "    w = dims[0,1].item()\n",
    "    \n",
    "    # Remove last remainder rows if image height is not multiples of 8x8\n",
    "    if h % 8 != 0:\n",
    "        new_h = h // 8 * 8\n",
    "        dct_blocks = dct_blocks[:-1]  # already in blocks of 8, drop last\n",
    "        gt_mask = gt_mask[:new_h]\n",
    "        \n",
    "    # Do the same for columns\n",
    "    if w % 8 != 0:\n",
    "        new_w = w // 8 * 8\n",
    "        dct_blocks = dct_blocks[:,:-1]  # already is blocks of 8, drop last\n",
    "        gt_mask = gt_mask[:,:new_w]\n",
    "\n",
    "    # Blockify ground truth image\n",
    "    gt_include_edge = torch.nn.functional.max_pool2d(gt_mask[(None,)*2], kernel_size=(8,8)).squeeze()\n",
    "    gt_exclude_edge = (-torch.nn.functional.max_pool2d(-gt_mask[(None,)*2], kernel_size=(8,8))).squeeze()\n",
    "    \n",
    "    \n",
    "    # Retrieve clean and tampered blocks\n",
    "    tampered_dct_blocks = dct_blocks[gt_exclude_edge.bool()]\n",
    "    clean_dct_blocks = dct_blocks[(1 - gt_include_edge).bool()]\n",
    "\n",
    "    return tampered_dct_blocks, clean_dct_blocks\n",
    "\n",
    "\n",
    "def compute_cropped_dct_blocks(tampered_path, gt_path):\n",
    "    # Read tampered image into spatial dimension and crop\n",
    "    tampered_cropped = to_tensor(crop_leave4(Image.open(tampered_path).convert('L'))).squeeze()\n",
    "        \n",
    "    # Read ground truth mask and crop\n",
    "    gt_cropped = to_tensor(crop_leave4(Image.open(gt_path).convert('L'))).squeeze()\n",
    "    \n",
    "    # Remove last remainder rows if image height is not multiples of 8x8\n",
    "    if tampered_cropped.size(0) % 8 != 0:\n",
    "        h = tampered_cropped.size(0) // 8 * 8\n",
    "        tampered_cropped = tampered_cropped[:h]\n",
    "        gt_cropped = gt_cropped[:h]\n",
    "        \n",
    "    # Do the same for columns\n",
    "    if tampered_cropped.size(1) % 8 != 0:\n",
    "        w = tampered_cropped.size(1) // 8 * 8\n",
    "        tampered_cropped = tampered_cropped[:,:w]\n",
    "        gt_cropped = gt_cropped[:,:w]\n",
    "\n",
    "    # Process tampered image pixels into dct blocks\n",
    "    pixels_blocks = ((tampered_cropped * 255) - 128).squeeze().unfold(0, 8, 8).unfold(1, 8, 8).reshape(-1, 8, 8)\n",
    "    dct_blocks = torchjpeg.dct.block_dct(pixels_blocks[(None,)*2]).squeeze()\n",
    "\n",
    "    # Blockify ground truth image\n",
    "    gt_include_edge = torch.nn.functional.max_pool2d(gt_cropped[(None,)], kernel_size=(8,8)).squeeze().reshape(-1)\n",
    "    gt_exclude_edge = (-torch.nn.functional.max_pool2d(-gt_cropped[(None,)], kernel_size=(8,8))).squeeze().reshape(-1)\n",
    "\n",
    "    # Retrieve clean and tampered blocks\n",
    "    tampered_dct_blocks = dct_blocks[gt_exclude_edge.bool()]\n",
    "    clean_dct_blocks = dct_blocks[(1 - gt_include_edge).bool()]\n",
    "    \n",
    "    return tampered_dct_blocks, clean_dct_blocks\n",
    "\n",
    "def compression_simulation_for_dct_blocks(dct_blocks, n, mth_q_table, bin_range=100):\n",
    "    \"\"\"\n",
    "    3. Simulate compressions with n constant matrices using DCT coefficients\n",
    "        1. Perform quantization with chosen constant quantization table. This procedure is lossy. \n",
    "        2. Dequantize coefficients by multiplying with the same constant quantization table.\n",
    "        3. Repeat steps 2.C.a and 2.C.b with m-th quantization table\n",
    "        4. After the \"simulated\" compressions (steps 2.C.a-b), compute the DCT coefficient histogram.\n",
    "    \n",
    "    Args:\n",
    "        im: PIL image.\n",
    "        n: Number of constant matrices to try.\n",
    "        mth_q_table: Quantization table used in the m-th compression.\n",
    "    Returns:\n",
    "        List of list of histograms, of shape (64, n, histogram_size). \n",
    "    \"\"\"    \n",
    "    ############################################\n",
    "    # Step 3: Create n constant matrices and do compression (quantize and dequantize)\n",
    "    ############################################\n",
    "    k_hists_compare = []\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        # Create constant matrix with element i\n",
    "        # M_i is just a length 64 constant array since jpeg compress takes in a 1d array\n",
    "        M_i = torch.ones((8,8)) * i\n",
    "        \n",
    "        ############################################\n",
    "        # Step 3.1 & 3.2: \"Compress\" using M_i\n",
    "        ############################################\n",
    "        quantized_dct_blocks = torch.round(dct_blocks / M_i)  # lossy step\n",
    "        dequantized_dct_blocks = quantized_dct_blocks * M_i\n",
    "        \n",
    "        ############################################\n",
    "        # Step 3.3: \"Compress\" again using mth_q_table\n",
    "        ############################################\n",
    "        quantized_dct_blocks = torch.round(dequantized_dct_blocks / mth_q_table)  # lossy step\n",
    "        dequantized_dct_blocks = quantized_dct_blocks * mth_q_table\n",
    "        \n",
    "        ############################################\n",
    "        # Step 3.4: Compute 64 histograms from dequantized dct coefficients\n",
    "        ############################################\n",
    "        k_hists = compute_dct_coefficient_histogram(np.array(dequantized_dct_blocks.squeeze()), bin_range=bin_range)\n",
    "        k_hists_compare.append(k_hists)\n",
    "        \n",
    "    k_hists_compare = np.array(k_hists_compare).transpose(1,0,2)\n",
    "    return k_hists_compare\n",
    "\n",
    "def estimate_q_table_from_dct_blocks(dct_blocks, dct_blocks_cropped, mth_q_table, n, bin_range):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        im_path: Path to m-compressed image. Assume image is grayscale only.\n",
    "        n: Greatest value assumed by quantization factors\n",
    "    \"\"\"\n",
    "    # Step 1: Compute reference k DCT histograms for m-compressed image    \n",
    "    k_hists_ref = compute_dct_coefficient_histogram(dct_blocks.numpy(), bin_range=bin_range)\n",
    "    \n",
    "    # Step 2: Simulate compressions to get k factor histograms\n",
    "    k_hists_compare = compression_simulation_for_dct_blocks(dct_blocks_cropped.numpy(), n, mth_q_table, bin_range=bin_range)\n",
    "    \n",
    "    # Step 3: Compute closest histogram using chi-square histogram distance\n",
    "    estimation = np.zeros(64)\n",
    "    for i in range(64):\n",
    "        hist, idx = get_closest_histogram(k_hists_ref[i], k_hists_compare[i])\n",
    "        best_n = idx + 1  # since n starts from 1\n",
    "        estimation[i] = best_n\n",
    "    \n",
    "    return estimation.reshape((8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4665453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_q_tables(tampered_path, ground_truth_path, n, bin_range):\n",
    "    # Get m-th quantization table\n",
    "    _, mth_q_tables, _, _ = torchjpeg.codec.read_coefficients(tampered_path)\n",
    "    mth_q_table_lumi = mth_q_tables[0]\n",
    "    \n",
    "    # Compute DCT blocks\n",
    "    tampered_dct_blocks, clean_dct_blocks = compute_dct_blocks(tampered_path, ground_truth_path)\n",
    "    cropped_tampered_dct_blocks, cropped_clean_dct_blocks = compute_cropped_dct_blocks(tampered_path, ground_truth_path)\n",
    "    \n",
    "    # Estimate q-tables\n",
    "    est_clean = estimate_q_table_from_dct_blocks(clean_dct_blocks, cropped_clean_dct_blocks, mth_q_table_lumi, n, bin_range)    \n",
    "    est_tampered = estimate_q_table_from_dct_blocks(tampered_dct_blocks, cropped_tampered_dct_blocks, mth_q_table_lumi, n, bin_range)\n",
    "    diff = get_first_n_percentage_diff(est_clean, est_tampered, 15)\n",
    "    # 计算余弦相似度\n",
    "\n",
    "\n",
    "    \n",
    "    #cos_sim = torch.nn.functional.cosine_similarity(torch.from_numpy(est_clean).view(1, -1), torch.from_numpy(est_tampered).view(1, -1), dim=1)\n",
    "\n",
    "    # 计算欧氏距离\n",
    "    #euclidean_dist = torch.norm(a-b)\n",
    "\n",
    "    #print(\"Cosine Similarity: \", cos_sim.item())\n",
    "    #print(\"Euclidean Distance: \", euclidean_dist.item())\n",
    "\n",
    "    \n",
    "    return est_clean, est_tampered, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "055fe5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff:46.666666666666664\n"
     ]
    }
   ],
   "source": [
    "whole_img = image_dir\n",
    "tmp_img = comb_mask_dir\n",
    "\n",
    "est_clean, est_tampered, diff = predict_q_tables(whole_img,tmp_img, 100, 100)\n",
    "print(\"diff:{}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93282ac8",
   "metadata": {},
   "source": [
    "comp30.jpg 100\n",
    "\n",
    "splicing_PQ10.jpg 13.333333333333334(out) 13.333333333333334(in) \n",
    "\n",
    "copy_move_PQ10  0.0(out) \n",
    "\n",
    "im40_edit1.jpg 26.666666666666668\n",
    "\n",
    "comp26912.jpg  diff:60.0\n",
    "\n",
    "im41_edit1  53.333333333333336\n",
    "\n",
    "im41_edit2 40.0\n",
    "\n",
    "im41_edit3  46.666666666666664\n",
    "\n",
    "im42_edit1 80.0\n",
    "\n",
    "im42_edit2 20.0\n",
    "\n",
    "im42_edit3 46.666666666666664\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab10c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  1.,  4.,  1.,  2.,  9.,  3.],\n",
       "       [ 2.,  2.,  2.,  4.,  2.,  3.,  1.,  2.],\n",
       "       [ 1.,  2.,  3.,  2.,  3.,  4.,  3., 13.],\n",
       "       [ 4.,  4.,  2.,  1.,  4.,  5., 11., 13.],\n",
       "       [ 2.,  2.,  3.,  4.,  5.,  5., 13.,  4.],\n",
       "       [ 2.,  5.,  5.,  7., 10.,  4.,  4., 14.],\n",
       "       [ 1.,  5.,  3.,  3., 11., 11., 11., 14.],\n",
       "       [ 5.,  5.,  5.,  6.,  6.,  6., 14., 14.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a1e01a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  1.,  4.,  1.,  2.,  8.,  2.],\n",
       "       [ 2.,  2.,  2.,  4.,  1.,  8.,  1.,  1.],\n",
       "       [ 1.,  2.,  1.,  1.,  1., 11.,  3., 15.],\n",
       "       [ 4.,  4.,  1.,  1.,  2.,  5., 11., 14.],\n",
       "       [ 1.,  1.,  1.,  2., 13., 17., 11.,  1.],\n",
       "       [ 2.,  1.,  8.,  8., 10.,  4.,  4.,  2.],\n",
       "       [ 8.,  8.,  3.,  6., 10., 11., 10.,  3.],\n",
       "       [ 5.,  5., 17., 17., 39., 17.,  1.,  3.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_tampered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee47072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wavelet] *",
   "language": "python",
   "name": "conda-env-wavelet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
